
# 생성형 AI의 전체 동작 과정

> 텍스트 기반 GPT 계열 모델을 예로 설명하지만, 이미지·음성·영상 모델에도 거의 비슷하게 적용됩니다.

---

## 1. 데이터 수집 & 전처리
- **목적**: 모델이 학습할 원천 데이터를 준비
- **내용**
  - 인터넷, 책, 코드, 이미지, 음성 등 다양한 출처에서 **대규모 데이터** 수집
  - 품질 관리: 불필요한 노이즈, 중복, 오류 제거
  - 포맷 통일: 토큰화(tokenization), 해상도 조정, 오디오 샘플링 등
- **예시**
  - 텍스트 모델: 뉴스, 위키피디아, 책, 포럼 글 등
  - 이미지 모델: 라벨이 있는 사진, 그림, 데이터셋

---

## 2. 모델 아키텍처 설계
- **대표 구조**: Transformer
  - **Encoder**: 입력 이해 (주로 번역·요약 등)
  - **Decoder**: 출력 생성 (텍스트·이미지 등)
  - **Encoder-Decoder**: 이해+생성 모두 필요할 때
- 각 층에는 **Self-Attention**과 **Feed Forward Network**가 반복 배치
- 모델 크기는 파라미터 수(예: 7B, 13B, 70B)로 구분

---

## 3. 사전 학습 (Pretraining)
- **목적**: 세상 일반 지식을 모델에 담기
- **방식**
  - 대규모 데이터에서 다음 단어 예측(Next Token Prediction) 또는 마스크 예측(Masked Language Modeling)
  - 손실 함수: Cross-Entropy Loss 사용
- **특징**
  - 언어 패턴, 사실, 문법, 스타일 학습
  - 특정 업무에는 최적화 안 되어 있음

---

## 4. 미세 조정 (Fine-tuning)
- **목적**: 모델을 특정 업무에 맞춤
- **방법**
  1. **Supervised Fine-tuning (SFT)**: 사람이 작성한 Q→A 데이터로 훈련
  2. **Instruction tuning**: “질문-지시-응답” 형식으로 학습
  3. **Domain-specific fine-tuning**: 의료, 법률, 게임 대사 등 특수 분야
- **장점**
  - 특정 주제 정확도 향상
  - 모델 응답 스타일 통일

---

## 5. 인간 피드백 기반 강화학습 (RLHF)
- **목적**: 안전하고 유용한 응답 생성
- **과정**
  1. 여러 응답 생성
  2. 사람이 “좋은 순서”로 랭킹
  3. 보상 모델(Reward Model) 학습
  4. 정책 최적화(PPO 등)로 모델 업데이트
- **결과**
  - 유해 발언 감소
  - 사용자가 선호하는 스타일 강화

---

## 6. 추론 (Inference) 과정
1. **프롬프트 입력**
   - 예: “달 표면에 사는 토끼에 대한 시 써줘”
2. **토큰화(Tokenization)**
   - 문장을 숫자 ID 시퀀스로 변환
3. **모델 내부 연산**
   - Self-Attention으로 모든 토큰 간 관계 계산
   - 다음 토큰 확률 분포 생성
4. **샘플링(Decoding)**
   - Greedy Search, Beam Search, Top-k, Top-p, Temperature 등을 사용
   - 하나씩 토큰 생성 → 최종 문장 완성
5. **디토큰화(Detokenization)**
   - 숫자를 다시 단어·문장으로 변환

---

## 7. 출력 후처리
- **필터링**
  - 금지어, 개인정보, 유해 표현 제거
- **형식화**
  - JSON, 마크다운, HTML 등 구조화
- **사용자 맞춤**
  - 번역, 요약, 말투 조절

---

## 8. 피드백 & 지속적 개선
- 실제 사용자 데이터 분석
- 문제 사례(할루시네이션, 편향, 오류) 수집
- 추가 학습 또는 안전 필터 업데이트

---

## 📌 전체 흐름도
```
데이터 수집 → 전처리 → 사전 학습 → 미세 조정 → RLHF
     ↓
   추론 단계 (프롬프트 입력 → 토큰화 → 모델 연산 → 샘플링 → 출력)
     ↓
 후처리 & 사용자 전달 → 피드백 → 재학습
```
